import redis
import time
from langchain_community.llms import Ollama

# 1. åˆå§‹åŒ–ï¼šé€£æ¥æœ¬åœ° AI å¼•æ“å’Œ Redis
print("ğŸ¤– AI Agent æ­£åœ¨å•Ÿå‹•ï¼Œè«‹ç¨å€™...")
try:
    llm = Ollama(model="llama3.2:1b")
    r = redis.Redis(host='localhost', port=6379, decode_responses=True)
    print("âœ… AI å¼•æ“èˆ‡æ•¸æ“šé€šé“é€£æ¥æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ å•Ÿå‹•å¤±æ•—: {e}")
    exit()

def analyze_risk():
    # 2. å¾ Redis è®€å–ç›£æ§ç‹€æ…‹
    status = r.get("circuit_breaker:status")
    
    if status == "OPEN":
        reason = r.get("circuit_breaker:reason")
        print(f"\nğŸš¨ [ç·Šæ€¥è­¦å ±] æª¢æ¸¬åˆ° Web3 ç†”æ–·ï¼")
        print(f"ğŸ“ è§¸ç™¼åŸå› : {reason}")
        
        # 3. è«‹æ±‚ AI æä¾›å°ˆå®¶å»ºè­°
        prompt = f"""
        ä½ æ˜¯ä¸€ä½é ‚ç´š Web3 å®‰å…¨æ¶æ§‹å¸«ã€‚
        ç•¶å‰ç³»çµ±è§¸ç™¼äº†ç†”æ–·æ©Ÿåˆ¶ï¼ŒåŸå› æ˜¯ï¼š{reason}ã€‚
        è«‹ç”¨ä¸­æ–‡çµ¦å‡º 3 æ¢å…·é«”çš„æ‡‰æ€¥æ“ä½œå»ºè­°ï¼Œä»¥ä¿è­·å”è­°è³‡ç”¢å®‰å…¨ã€‚
        """
        
        print("ğŸ¤” AI æ­£åœ¨é€²è¡Œé¢¨éšªå»ºæ¨¡èˆ‡åˆ†æ...")
        response = llm.invoke(prompt)
        print("\n--- ğŸ›¡ï¸ AI å°ˆå®¶æ‡‰æ€¥å»ºè­° ---")
        print(response)
        print("--------------------------")
    else:
        print(f"ğŸŸ¢ {time.strftime('%H:%M:%S')} ç›£æ§ä¸­ï¼šç³»çµ±é‹è¡Œå¹³ç©©ï¼Œç„¡ç•°å¸¸é¢¨éšªã€‚")

if __name__ == "__main__":
    # æ¯ 15 ç§’æª¢æŸ¥ä¸€æ¬¡
    while True:
        analyze_risk()
        time.sleep(15)
